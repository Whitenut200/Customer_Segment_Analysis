from pyspark.sql import SparkSession, functions as F
from functools import reduce
from delta import configure_spark_with_delta_pip
from pyspark.sql import SparkSession

# DB ì •ë³´
db_url = "jdbc:postgresql://localhost:5432/postgres"
# db_table = "public.REM_base"
db_user = ""
db_password = ""
db_driver = "org.postgresql.Driver"

# Spark / Delta ì„¸ì…˜
builder = (
    SparkSession.builder
    .appName("delta-metrics")
    .master("local[*]")  # ë¡œì»¬ ì „ì²´ ì½”ì–´ ì‚¬ìš©
    # ğŸ”¹ Delta Lake ì„¤ì • (ê¸°ì¡´ ê·¸ëŒ€ë¡œ ìœ ì§€)
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
    # ğŸ”¹ JDBC(PostgreSQL) ë“œë¼ì´ë²„
    .config("spark.jars", r"D:\project\segment\postgresql-42.7.8.jar")
    # ğŸ”¹ ë©”ëª¨ë¦¬/ì„±ëŠ¥ ì„¤ì • ì¶”ê°€
    .config("spark.driver.memory", "12g")              # ë“œë¼ì´ë²„ ë©”ëª¨ë¦¬ ì¦ê°€
    .config("spark.sql.shuffle.partitions", "300")     # ì…”í”Œ íŒŒí‹°ì…˜ ìˆ˜ ì¡°ì •
)

spark = configure_spark_with_delta_pip(builder).getOrCreate()

DELTA_PATH = "D:/project/segment/warehouse/events_delta"

# 1. Delta ì½ê¸°
events_df = (
    spark.read
         .format("delta")
         .load(DELTA_PATH)
)

snapshot_months = [
    "2019-10-31", "2019-11-30", "2019-12-31",
    "2020-01-31", "2020-02-29", "2020-03-31", "2020-04-30"
]
all_data = []

for snap in snapshot_months:

    # snapshot ê¸°ì¤€ ì´ì „ê¹Œì§€ ë°ì´í„° (ëˆ„ì ) - Recencyìš©
    history_df = events_df.filter(F.to_date("event_time") <= F.lit(snap))
    history_df.createOrReplaceTempView("hist_events")

    # Recency ê³„ì‚° (ëˆ„ì  ê¸°ì¤€)
    # standard_date = hist_eventsì—ì„œ ê°€ì¥ ë§ˆì§€ë§‰ êµ¬ë§¤ì¼ + 1ì¼
    last_purchase_sql = """
    WITH standard_date AS (
        SELECT 
            date_add(MAX(to_date(event_time)), 1) AS standard_date
        FROM hist_events
        WHERE event_type = 'purchase'
    ),
    last_purchase AS (
        SELECT
            user_id,
            MAX(to_date(event_time)) AS last_purchase
        FROM hist_events
        WHERE event_type = 'purchase'
        GROUP BY user_id
    )
    SELECT 
        l.user_id,
        s.standard_date,
        datediff(s.standard_date, l.last_purchase) AS recency
    FROM last_purchase l
    CROSS JOIN standard_date s
    """
    recency_df = spark.sql(last_purchase_sql)

    # í•´ë‹¹ ì›”ì˜ F/M ê³„ì‚° (ë‹¹ì›” ê¸°ì¤€)
    # snapì´ '2019-11-30'ì´ë©´ 2019-11 ì „ì²´ë§Œ í¬í•¨
    RFM_base_monthly = f"""
    WITH month_stat AS (
        SELECT
            user_id,
            COUNT(*)  AS month_freq,
            SUM(price) AS month_monetary
        FROM hist_events
        WHERE event_type = 'purchase'
          AND date_trunc('month', to_date(event_time)) 
              = date_trunc('month', DATE '{snap}')
        GROUP BY user_id
    )
    SELECT * FROM month_stat
    """
    monthly_FM_df = spark.sql(RFM_base_monthly)

    # Recency + F/M ì¡°ì¸ + snapshot_date ì»¬ëŸ¼ ì¶”ê°€
    base_df = (
        monthly_FM_df
        .join(recency_df, on="user_id", how="left")
        .withColumn("snapshot_date", F.lit(snap))
    )

    all_data.append(base_df)

# ëª¨ë“  ìŠ¤ëƒ…ìƒ· Union
if not all_data:
    raise ValueError("ìŠ¤ëƒ…ìƒ· ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

monthly_rfm_base_df = reduce(lambda df1, df2: df1.unionByName(df2), all_data)
monthly_rfm_base_df.createOrReplaceTempView("monthly_rfm_base")

# RFM ìŠ¤ì½”ì–´ + ì„¸ê·¸ë¨¼íŠ¸ ë¶„ë¥˜
monthly_RFM_SQL = """
WITH rfm_score AS (
    SELECT
        user_id,
        snapshot_date,
        recency,
        month_freq,
        month_monetary,
        ntile(5) OVER (
            PARTITION BY snapshot_date 
            ORDER BY recency DESC
        ) AS recency_score,
        ntile(5) OVER (
            PARTITION BY snapshot_date 
            ORDER BY month_freq ASC
        ) AS frequency_score,
        ntile(5) OVER (
            PARTITION BY snapshot_date 
            ORDER BY month_monetary ASC
        ) AS monetary_score
    FROM monthly_rfm_base
),
rfm_user AS (
    SELECT 
        user_id,
        snapshot_date,
        CASE
            WHEN recency_score = 5 AND frequency_score = 5 AND monetary_score = 5 
                THEN 'VIP'
            WHEN recency_score >= 4 AND frequency_score >= 4 AND monetary_score >= 4
                THEN 'ìš°ìˆ˜ ê³ ê°'
            WHEN recency_score >= 3 AND frequency_score >= 3 AND monetary_score >= 3
                THEN 'ì„±ì¥ ê³ ê°'
            WHEN recency_score = 5 AND frequency_score = 1
                THEN 'ì‹ ê·œ ê³ ê°'
            WHEN recency_score = 1 AND frequency_score <= 2
                THEN 'ì´íƒˆ ê³ ìœ„í—˜ ê³ ê°'
            WHEN recency_score = 2 AND frequency_score <= 2
                THEN 'ì´íƒˆ ì˜ˆì • ê³ ê°'
            ELSE 'ì¼ë°˜ ê³ ê°'
        END AS rfm_segment,
        recency,
        recency_score,
        month_freq,
        frequency_score,
        month_monetary,
        monetary_score
    FROM rfm_score
)
SELECT * FROM rfm_user
"""

monthly_RFM_df = spark.sql(monthly_RFM_SQL)

# PostgreSQL ì ì¬
try:
    print("PostgreSQL ì ì¬ ì‹œì‘")

    (monthly_RFM_df
        .write
        .format("jdbc")
        .option("url", db_url)
        .option("dbtable", "new_monthly_rfm")   # í•˜ë‚˜ì˜ í…Œì´ë¸”ì— ì›”ë³„ append
        .option("user", db_user)
        .option("password", db_password)
        .option("driver", db_driver)
        .mode("append")
        .save()
    )

    print("monthly_rfm ì „ì²´ ì ì¬ ì™„ë£Œ")

except Exception as e:
    print("ì ì¬ ì‹¤íŒ¨")
    print(e)
